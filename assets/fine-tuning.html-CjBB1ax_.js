import{_ as i,c as e,b as a,o as n}from"./app-BvwJZ6kh.js";const t={};function s(r,l){return n(),e("div",null,[...l[0]||(l[0]=[a(`<h1 id="大模型微调-fine-tuning-学习笔记" tabindex="-1"><a class="header-anchor" href="#大模型微调-fine-tuning-学习笔记"><span>大模型微调(Fine-tuning)学习笔记</span></a></h1><h2 id="什么是大模型微调" tabindex="-1"><a class="header-anchor" href="#什么是大模型微调"><span>什么是大模型微调</span></a></h2><p>大模型微调是指在预训练的大语言模型基础上，使用特定领域或任务的数据进行进一步训练，以使模型更好地适应特定应用场景的过程。相比于从头开始训练模型，微调能够显著减少所需的计算资源和时间。</p><h2 id="微调与提示工程的比较" tabindex="-1"><a class="header-anchor" href="#微调与提示工程的比较"><span>微调与提示工程的比较</span></a></h2><table><thead><tr><th>方面</th><th>提示工程</th><th>微调</th></tr></thead><tbody><tr><td>成本</td><td>低（无需训练）</td><td>高（需要计算资源）</td></tr><tr><td>时间</td><td>即时</td><td>需要训练时间</td></tr><tr><td>效果</td><td>依赖于提示质量</td><td>通常效果更好</td></tr><tr><td>通用性</td><td>适用于任何模型</td><td>需要针对具体模型</td></tr><tr><td>可移植性</td><td>高</td><td>低（需保存模型）</td></tr></tbody></table><h2 id="微调的基本原理" tabindex="-1"><a class="header-anchor" href="#微调的基本原理"><span>微调的基本原理</span></a></h2><h3 id="_1-迁移学习" tabindex="-1"><a class="header-anchor" href="#_1-迁移学习"><span>1. 迁移学习</span></a></h3><ul><li>利用预训练模型学到的通用知识</li><li>在特定任务上进行适应性调整</li></ul><h3 id="_2-参数更新" tabindex="-1"><a class="header-anchor" href="#_2-参数更新"><span>2. 参数更新</span></a></h3><ul><li>冻结部分层，只训练顶层</li><li>或者全部参数一起训练</li></ul><h3 id="_3-学习率设置" tabindex="-1"><a class="header-anchor" href="#_3-学习率设置"><span>3. 学习率设置</span></a></h3><ul><li>通常使用较小的学习率</li><li>避免破坏预训练模型的知识</li></ul><h2 id="微调方法" tabindex="-1"><a class="header-anchor" href="#微调方法"><span>微调方法</span></a></h2><h3 id="_1-全参数微调-full-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_1-全参数微调-full-fine-tuning"><span>1. 全参数微调(Full Fine-tuning)</span></a></h3><p>更新模型的所有参数，适用于：</p><ul><li>有足够的计算资源</li><li>有大量高质量的训练数据</li><li>需要最大化的性能提升</li></ul><p>优点：</p><ul><li>性能提升最为显著</li><li>模型适应性最强</li></ul><p>缺点：</p><ul><li>计算成本高</li><li>容易过拟合</li><li>存储开销大</li></ul><h3 id="_2-部分参数微调" tabindex="-1"><a class="header-anchor" href="#_2-部分参数微调"><span>2. 部分参数微调</span></a></h3><p>只更新模型的部分参数：</p><h4 id="a-layer-freezing" tabindex="-1"><a class="header-anchor" href="#a-layer-freezing"><span>a. Layer Freezing</span></a></h4><p>冻结底层参数，只训练顶层：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line"># 示例：冻结前6层</span>
<span class="line">for param in model.parameters():</span>
<span class="line">    param.requires_grad = False</span>
<span class="line">    </span>
<span class="line"># 只训练最后几层</span>
<span class="line">for param in model.layers[-2:].parameters():</span>
<span class="line">    param.requires_grad = True</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="b-bias-only-fine-tuning" tabindex="-1"><a class="header-anchor" href="#b-bias-only-fine-tuning"><span>b. Bias-only Fine-tuning</span></a></h4><p>只训练偏置参数，保持权重不变。</p><h3 id="_3-适配器微调-adapter-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_3-适配器微调-adapter-fine-tuning"><span>3. 适配器微调(Adapter Fine-tuning)</span></a></h3><p>在模型的每一层中插入小型神经网络模块(适配器)：</p><p>优点：</p><ul><li>参数效率高</li><li>易于部署多个适配器</li><li>不改变原模型结构</li></ul><p>缺点：</p><ul><li>训练和推理时会有额外延迟</li><li>性能可能略低于全参数微调</li></ul><h3 id="_4-lora-low-rank-adaptation" tabindex="-1"><a class="header-anchor" href="#_4-lora-low-rank-adaptation"><span>4. LoRA (Low-Rank Adaptation)</span></a></h3><p>将权重更新矩阵分解为低秩矩阵的乘积：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">ΔW = A × B</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>其中A和B是低秩矩阵，参数量远小于原始权重矩阵。</p><p>优点：</p><ul><li>极大地减少了可训练参数</li><li>训练完成后可以合并到原模型中</li><li>易于切换不同的适配模块</li></ul><p>缺点：</p><ul><li>需要修改模型结构</li><li>对某些任务效果可能不如全参数微调</li></ul><h3 id="_5-prompt-tuning" tabindex="-1"><a class="header-anchor" href="#_5-prompt-tuning"><span>5. Prompt Tuning</span></a></h3><p>只训练输入提示部分的参数向量，而不是模型本身：</p><p>优点：</p><ul><li>参数效率极高</li><li>保持模型不变</li><li>易于部署</li></ul><p>缺点：</p><ul><li>对于复杂任务效果有限</li><li>需要较长的提示序列</li></ul><h3 id="_6-prefix-tuning" tabindex="-1"><a class="header-anchor" href="#_6-prefix-tuning"><span>6. Prefix Tuning</span></a></h3><p>在每一层的输入前添加可学习的前缀向量：</p><p>优点：</p><ul><li>比Prompt Tuning更强大</li><li>逐层控制模型行为</li></ul><p>缺点：</p><ul><li>实现相对复杂</li><li>需要调整模型输入</li></ul><h2 id="微调步骤" tabindex="-1"><a class="header-anchor" href="#微调步骤"><span>微调步骤</span></a></h2><h3 id="_1-数据准备" tabindex="-1"><a class="header-anchor" href="#_1-数据准备"><span>1. 数据准备</span></a></h3><ul><li>收集和清洗训练数据</li><li>格式化为模型输入格式</li><li>划分训练集、验证集和测试集</li></ul><h3 id="_2-模型选择" tabindex="-1"><a class="header-anchor" href="#_2-模型选择"><span>2. 模型选择</span></a></h3><ul><li>选择合适的预训练模型</li><li>考虑模型大小和计算资源</li><li>考虑任务特点和数据量</li></ul><h3 id="_3-微调设置" tabindex="-1"><a class="header-anchor" href="#_3-微调设置"><span>3. 微调设置</span></a></h3><ul><li>设置学习率和优化器</li><li>选择批次大小和训练轮数</li><li>设置早停机制防止过拟合</li></ul><h3 id="_4-训练过程" tabindex="-1"><a class="header-anchor" href="#_4-训练过程"><span>4. 训练过程</span></a></h3><ul><li>监控训练损失和验证指标</li><li>调整超参数</li><li>保存最佳模型</li></ul><h3 id="_5-评估和部署" tabindex="-1"><a class="header-anchor" href="#_5-评估和部署"><span>5. 评估和部署</span></a></h3><ul><li>在测试集上评估性能</li><li>部署到生产环境</li><li>持续监控和优化</li></ul><h2 id="微调关键技术" tabindex="-1"><a class="header-anchor" href="#微调关键技术"><span>微调关键技术</span></a></h2><h3 id="_1-学习率调度" tabindex="-1"><a class="header-anchor" href="#_1-学习率调度"><span>1. 学习率调度</span></a></h3><ul><li>预热(warmup)策略</li><li>余弦退火(cosine annealing)</li><li>分层学习率(layer-wise learning rate)</li></ul><h3 id="_2-正则化技术" tabindex="-1"><a class="header-anchor" href="#_2-正则化技术"><span>2. 正则化技术</span></a></h3><ul><li>Dropout防止过拟合</li><li>权重衰减(weight decay)</li><li>梯度裁剪(gradient clipping)</li></ul><h3 id="_3-批量大小选择" tabindex="-1"><a class="header-anchor" href="#_3-批量大小选择"><span>3. 批量大小选择</span></a></h3><ul><li>根据GPU内存确定</li><li>影响训练稳定性和收敛速度</li><li>可使用梯度累积模拟大批量训练</li></ul><h3 id="_4-混合精度训练" tabindex="-1"><a class="header-anchor" href="#_4-混合精度训练"><span>4. 混合精度训练</span></a></h3><ul><li>使用FP16减少内存占用</li><li>加速训练过程</li><li>保持模型精度</li></ul><h2 id="微调最佳实践" tabindex="-1"><a class="header-anchor" href="#微调最佳实践"><span>微调最佳实践</span></a></h2><h3 id="_1-数据质量优于数量" tabindex="-1"><a class="header-anchor" href="#_1-数据质量优于数量"><span>1. 数据质量优于数量</span></a></h3><ul><li>高质量的小数据集往往比低质量的大数据集效果更好</li><li>注重数据的多样性和代表性</li></ul><h3 id="_2-渐进式微调" tabindex="-1"><a class="header-anchor" href="#_2-渐进式微调"><span>2. 渐进式微调</span></a></h3><ul><li>先使用较小学习率进行初步微调</li><li>再使用更大学习率进行精细调整</li></ul><h3 id="_3-多任务学习" tabindex="-1"><a class="header-anchor" href="#_3-多任务学习"><span>3. 多任务学习</span></a></h3><ul><li>在相关任务上联合微调</li><li>提高模型的泛化能力</li></ul><h3 id="_4-正则化策略" tabindex="-1"><a class="header-anchor" href="#_4-正则化策略"><span>4. 正则化策略</span></a></h3><ul><li>合理使用Dropout</li><li>设置适当的学习率衰减</li><li>使用早停机制</li></ul><h3 id="_5-监控和调试" tabindex="-1"><a class="header-anchor" href="#_5-监控和调试"><span>5. 监控和调试</span></a></h3><ul><li>实时监控训练曲线</li><li>使用可视化工具分析注意力权重</li><li>定期评估验证集性能</li></ul><h2 id="微调评估指标" tabindex="-1"><a class="header-anchor" href="#微调评估指标"><span>微调评估指标</span></a></h2><h3 id="_1-任务特定指标" tabindex="-1"><a class="header-anchor" href="#_1-任务特定指标"><span>1. 任务特定指标</span></a></h3><ul><li>分类任务：准确率、F1分数、AUC</li><li>生成任务：BLEU、ROUGE、METEOR</li><li>问答任务：EM(Exact Match)、F1</li></ul><h3 id="_2-通用指标" tabindex="-1"><a class="header-anchor" href="#_2-通用指标"><span>2. 通用指标</span></a></h3><ul><li>困惑度(Perplexity)</li><li>生成多样性</li><li>事实一致性</li></ul><h3 id="_3-效率指标" tabindex="-1"><a class="header-anchor" href="#_3-效率指标"><span>3. 效率指标</span></a></h3><ul><li>训练时间</li><li>推理延迟</li><li>模型大小</li></ul><h2 id="微调工具和框架" tabindex="-1"><a class="header-anchor" href="#微调工具和框架"><span>微调工具和框架</span></a></h2><h3 id="_1-hugging-face-transformers" tabindex="-1"><a class="header-anchor" href="#_1-hugging-face-transformers"><span>1. Hugging Face Transformers</span></a></h3><ul><li>最流行的NLP模型库</li><li>支持众多预训练模型</li><li>提供完整的微调工具链</li></ul><h3 id="_2-pytorch-lightning" tabindex="-1"><a class="header-anchor" href="#_2-pytorch-lightning"><span>2. PyTorch Lightning</span></a></h3><ul><li>简化PyTorch训练流程</li><li>内置最佳实践</li><li>支持分布式训练</li></ul><h3 id="_3-deepspeed" tabindex="-1"><a class="header-anchor" href="#_3-deepspeed"><span>3. DeepSpeed</span></a></h3><ul><li>微软的深度学习优化库</li><li>支持大规模模型训练</li><li>提供ZeRO优化器</li></ul><h3 id="_4-accelerate" tabindex="-1"><a class="header-anchor" href="#_4-accelerate"><span>4. Accelerate</span></a></h3><ul><li>Hugging Face的训练加速库</li><li>简化多GPU和TPU训练</li><li>自动处理设备放置</li></ul><h2 id="常见问题与解决方案" tabindex="-1"><a class="header-anchor" href="#常见问题与解决方案"><span>常见问题与解决方案</span></a></h2><h3 id="_1-内存不足" tabindex="-1"><a class="header-anchor" href="#_1-内存不足"><span>1. 内存不足</span></a></h3><p>解决方案：</p><ul><li>使用梯度检查点</li><li>减少批次大小</li><li>使用混合精度训练</li><li>应用模型并行</li></ul><h3 id="_2-过拟合" tabindex="-1"><a class="header-anchor" href="#_2-过拟合"><span>2. 过拟合</span></a></h3><p>解决方案：</p><ul><li>增加正则化</li><li>使用早停机制</li><li>增加训练数据</li><li>简化模型结构</li></ul><h3 id="_3-训练不稳定" tabindex="-1"><a class="header-anchor" href="#_3-训练不稳定"><span>3. 训练不稳定</span></a></h3><p>解决方案：</p><ul><li>降低学习率</li><li>使用学习率预热</li><li>检查数据质量</li><li>调整批次大小</li></ul><h3 id="_4-性能不佳" tabindex="-1"><a class="header-anchor" href="#_4-性能不佳"><span>4. 性能不佳</span></a></h3><p>解决方案：</p><ul><li>检查数据标注质量</li><li>调整超参数</li><li>尝试不同的微调方法</li><li>增加训练轮数</li></ul><h2 id="微调发展趋势" tabindex="-1"><a class="header-anchor" href="#微调发展趋势"><span>微调发展趋势</span></a></h2><h3 id="_1-参数高效微调" tabindex="-1"><a class="header-anchor" href="#_1-参数高效微调"><span>1. 参数高效微调</span></a></h3><ul><li>更多的参数高效方法涌现</li><li>在保持性能的同时减少参数量</li></ul><h3 id="_2-自适应微调" tabindex="-1"><a class="header-anchor" href="#_2-自适应微调"><span>2. 自适应微调</span></a></h3><ul><li>根据任务特点自动选择微调策略</li><li>动态调整微调参数</li></ul><h3 id="_3-多模态微调" tabindex="-1"><a class="header-anchor" href="#_3-多模态微调"><span>3. 多模态微调</span></a></h3><ul><li>文本、图像、音频等多模态联合微调</li><li>跨模态知识迁移</li></ul><h3 id="_4-联邦微调" tabindex="-1"><a class="header-anchor" href="#_4-联邦微调"><span>4. 联邦微调</span></a></h3><ul><li>在保护数据隐私的前提下进行微调</li><li>分布式协作训练</li></ul><p>大模型微调是一项复杂但非常有价值的技术，它能够让通用的大语言模型更好地适应特定的应用场景，从而在实际项目中发挥更大的作用。</p>`,123)])])}const d=i(t,[["render",s]]),p=JSON.parse('{"path":"/llm/fine-tuning.html","title":"大模型微调(Fine-tuning)学习笔记","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"什么是大模型微调","slug":"什么是大模型微调","link":"#什么是大模型微调","children":[]},{"level":2,"title":"微调与提示工程的比较","slug":"微调与提示工程的比较","link":"#微调与提示工程的比较","children":[]},{"level":2,"title":"微调的基本原理","slug":"微调的基本原理","link":"#微调的基本原理","children":[{"level":3,"title":"1. 迁移学习","slug":"_1-迁移学习","link":"#_1-迁移学习","children":[]},{"level":3,"title":"2. 参数更新","slug":"_2-参数更新","link":"#_2-参数更新","children":[]},{"level":3,"title":"3. 学习率设置","slug":"_3-学习率设置","link":"#_3-学习率设置","children":[]}]},{"level":2,"title":"微调方法","slug":"微调方法","link":"#微调方法","children":[{"level":3,"title":"1. 全参数微调(Full Fine-tuning)","slug":"_1-全参数微调-full-fine-tuning","link":"#_1-全参数微调-full-fine-tuning","children":[]},{"level":3,"title":"2. 部分参数微调","slug":"_2-部分参数微调","link":"#_2-部分参数微调","children":[]},{"level":3,"title":"3. 适配器微调(Adapter Fine-tuning)","slug":"_3-适配器微调-adapter-fine-tuning","link":"#_3-适配器微调-adapter-fine-tuning","children":[]},{"level":3,"title":"4. LoRA (Low-Rank Adaptation)","slug":"_4-lora-low-rank-adaptation","link":"#_4-lora-low-rank-adaptation","children":[]},{"level":3,"title":"5. Prompt Tuning","slug":"_5-prompt-tuning","link":"#_5-prompt-tuning","children":[]},{"level":3,"title":"6. Prefix Tuning","slug":"_6-prefix-tuning","link":"#_6-prefix-tuning","children":[]}]},{"level":2,"title":"微调步骤","slug":"微调步骤","link":"#微调步骤","children":[{"level":3,"title":"1. 数据准备","slug":"_1-数据准备","link":"#_1-数据准备","children":[]},{"level":3,"title":"2. 模型选择","slug":"_2-模型选择","link":"#_2-模型选择","children":[]},{"level":3,"title":"3. 微调设置","slug":"_3-微调设置","link":"#_3-微调设置","children":[]},{"level":3,"title":"4. 训练过程","slug":"_4-训练过程","link":"#_4-训练过程","children":[]},{"level":3,"title":"5. 评估和部署","slug":"_5-评估和部署","link":"#_5-评估和部署","children":[]}]},{"level":2,"title":"微调关键技术","slug":"微调关键技术","link":"#微调关键技术","children":[{"level":3,"title":"1. 学习率调度","slug":"_1-学习率调度","link":"#_1-学习率调度","children":[]},{"level":3,"title":"2. 正则化技术","slug":"_2-正则化技术","link":"#_2-正则化技术","children":[]},{"level":3,"title":"3. 批量大小选择","slug":"_3-批量大小选择","link":"#_3-批量大小选择","children":[]},{"level":3,"title":"4. 混合精度训练","slug":"_4-混合精度训练","link":"#_4-混合精度训练","children":[]}]},{"level":2,"title":"微调最佳实践","slug":"微调最佳实践","link":"#微调最佳实践","children":[{"level":3,"title":"1. 数据质量优于数量","slug":"_1-数据质量优于数量","link":"#_1-数据质量优于数量","children":[]},{"level":3,"title":"2. 渐进式微调","slug":"_2-渐进式微调","link":"#_2-渐进式微调","children":[]},{"level":3,"title":"3. 多任务学习","slug":"_3-多任务学习","link":"#_3-多任务学习","children":[]},{"level":3,"title":"4. 正则化策略","slug":"_4-正则化策略","link":"#_4-正则化策略","children":[]},{"level":3,"title":"5. 监控和调试","slug":"_5-监控和调试","link":"#_5-监控和调试","children":[]}]},{"level":2,"title":"微调评估指标","slug":"微调评估指标","link":"#微调评估指标","children":[{"level":3,"title":"1. 任务特定指标","slug":"_1-任务特定指标","link":"#_1-任务特定指标","children":[]},{"level":3,"title":"2. 通用指标","slug":"_2-通用指标","link":"#_2-通用指标","children":[]},{"level":3,"title":"3. 效率指标","slug":"_3-效率指标","link":"#_3-效率指标","children":[]}]},{"level":2,"title":"微调工具和框架","slug":"微调工具和框架","link":"#微调工具和框架","children":[{"level":3,"title":"1. Hugging Face Transformers","slug":"_1-hugging-face-transformers","link":"#_1-hugging-face-transformers","children":[]},{"level":3,"title":"2. PyTorch Lightning","slug":"_2-pytorch-lightning","link":"#_2-pytorch-lightning","children":[]},{"level":3,"title":"3. DeepSpeed","slug":"_3-deepspeed","link":"#_3-deepspeed","children":[]},{"level":3,"title":"4. Accelerate","slug":"_4-accelerate","link":"#_4-accelerate","children":[]}]},{"level":2,"title":"常见问题与解决方案","slug":"常见问题与解决方案","link":"#常见问题与解决方案","children":[{"level":3,"title":"1. 内存不足","slug":"_1-内存不足","link":"#_1-内存不足","children":[]},{"level":3,"title":"2. 过拟合","slug":"_2-过拟合","link":"#_2-过拟合","children":[]},{"level":3,"title":"3. 训练不稳定","slug":"_3-训练不稳定","link":"#_3-训练不稳定","children":[]},{"level":3,"title":"4. 性能不佳","slug":"_4-性能不佳","link":"#_4-性能不佳","children":[]}]},{"level":2,"title":"微调发展趋势","slug":"微调发展趋势","link":"#微调发展趋势","children":[{"level":3,"title":"1. 参数高效微调","slug":"_1-参数高效微调","link":"#_1-参数高效微调","children":[]},{"level":3,"title":"2. 自适应微调","slug":"_2-自适应微调","link":"#_2-自适应微调","children":[]},{"level":3,"title":"3. 多模态微调","slug":"_3-多模态微调","link":"#_3-多模态微调","children":[]},{"level":3,"title":"4. 联邦微调","slug":"_4-联邦微调","link":"#_4-联邦微调","children":[]}]}],"git":{"updatedTime":1765091269000,"contributors":[{"name":"mingwzh","username":"mingwzh","email":"1127699551@qq.com","commits":1,"url":"https://github.com/mingwzh"}],"changelog":[{"hash":"b058d4bca3374bab5807a7fe00edb3c38db9e19a","time":1765091269000,"email":"1127699551@qq.com","author":"mingwzh","message":"docs(llm): 添加大语言模型学习笔记文档"}]},"filePathRelative":"llm/fine-tuning.md","excerpt":"\\n<h2>什么是大模型微调</h2>\\n<p>大模型微调是指在预训练的大语言模型基础上，使用特定领域或任务的数据进行进一步训练，以使模型更好地适应特定应用场景的过程。相比于从头开始训练模型，微调能够显著减少所需的计算资源和时间。</p>\\n<h2>微调与提示工程的比较</h2>\\n<table>\\n<thead>\\n<tr>\\n<th>方面</th>\\n<th>提示工程</th>\\n<th>微调</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>成本</td>\\n<td>低（无需训练）</td>\\n<td>高（需要计算资源）</td>\\n</tr>\\n<tr>\\n<td>时间</td>\\n<td>即时</td>\\n<td>需要训练时间</td>\\n</tr>\\n<tr>\\n<td>效果</td>\\n<td>依赖于提示质量</td>\\n<td>通常效果更好</td>\\n</tr>\\n<tr>\\n<td>通用性</td>\\n<td>适用于任何模型</td>\\n<td>需要针对具体模型</td>\\n</tr>\\n<tr>\\n<td>可移植性</td>\\n<td>高</td>\\n<td>低（需保存模型）</td>\\n</tr>\\n</tbody>\\n</table>"}');export{d as comp,p as data};
