import comp from "/Users/mingwzh/IdeaProjects/PersonalWiki/docs/.vuepress/.temp/pages/bigdata/transformer-notes.html.vue"
const data = JSON.parse("{\"path\":\"/bigdata/transformer-notes.html\",\"title\":\"Transformer架构学习笔记\",\"lang\":\"en-US\",\"frontmatter\":{},\"headers\":[{\"level\":2,\"title\":\"Transformer简介\",\"slug\":\"transformer简介\",\"link\":\"#transformer简介\",\"children\":[]},{\"level\":2,\"title\":\"Transformer核心思想\",\"slug\":\"transformer核心思想\",\"link\":\"#transformer核心思想\",\"children\":[{\"level\":3,\"title\":\"1. 注意力机制\",\"slug\":\"_1-注意力机制\",\"link\":\"#_1-注意力机制\",\"children\":[]},{\"level\":3,\"title\":\"2. 编码器-解码器结构\",\"slug\":\"_2-编码器-解码器结构\",\"link\":\"#_2-编码器-解码器结构\",\"children\":[]}]},{\"level\":2,\"title\":\"Transformer整体架构\",\"slug\":\"transformer整体架构\",\"link\":\"#transformer整体架构\",\"children\":[{\"level\":3,\"title\":\"1. 编码器（Encoder）\",\"slug\":\"_1-编码器-encoder\",\"link\":\"#_1-编码器-encoder\",\"children\":[]},{\"level\":3,\"title\":\"2. 解码器（Decoder）\",\"slug\":\"_2-解码器-decoder\",\"link\":\"#_2-解码器-decoder\",\"children\":[]}]},{\"level\":2,\"title\":\"核心组件详解\",\"slug\":\"核心组件详解\",\"link\":\"#核心组件详解\",\"children\":[{\"level\":3,\"title\":\"1. 自注意力机制（Self-Attention）\",\"slug\":\"_1-自注意力机制-self-attention\",\"link\":\"#_1-自注意力机制-self-attention\",\"children\":[]},{\"level\":3,\"title\":\"2. 多头注意力（Multi-Head Attention）\",\"slug\":\"_2-多头注意力-multi-head-attention\",\"link\":\"#_2-多头注意力-multi-head-attention\",\"children\":[]},{\"level\":3,\"title\":\"3. 位置编码（Positional Encoding）\",\"slug\":\"_3-位置编码-positional-encoding\",\"link\":\"#_3-位置编码-positional-encoding\",\"children\":[]},{\"level\":3,\"title\":\"4. 位置前馈网络（Position-wise Feed-Forward Networks）\",\"slug\":\"_4-位置前馈网络-position-wise-feed-forward-networks\",\"link\":\"#_4-位置前馈网络-position-wise-feed-forward-networks\",\"children\":[]}]},{\"level\":2,\"title\":\"残差连接与层归一化\",\"slug\":\"残差连接与层归一化\",\"link\":\"#残差连接与层归一化\",\"children\":[{\"level\":3,\"title\":\"1. 残差连接（Residual Connection）\",\"slug\":\"_1-残差连接-residual-connection\",\"link\":\"#_1-残差连接-residual-connection\",\"children\":[]},{\"level\":3,\"title\":\"2. 层归一化（Layer Normalization）\",\"slug\":\"_2-层归一化-layer-normalization\",\"link\":\"#_2-层归一化-layer-normalization\",\"children\":[]},{\"level\":3,\"title\":\"3. 结构\",\"slug\":\"_3-结构\",\"link\":\"#_3-结构\",\"children\":[]}]},{\"level\":2,\"title\":\"解码器特殊设计\",\"slug\":\"解码器特殊设计\",\"link\":\"#解码器特殊设计\",\"children\":[{\"level\":3,\"title\":\"1. 掩码自注意力（Masked Self-Attention）\",\"slug\":\"_1-掩码自注意力-masked-self-attention\",\"link\":\"#_1-掩码自注意力-masked-self-attention\",\"children\":[]},{\"level\":3,\"title\":\"2. 编码器-解码器注意力\",\"slug\":\"_2-编码器-解码器注意力\",\"link\":\"#_2-编码器-解码器注意力\",\"children\":[]}]},{\"level\":2,\"title\":\"Transformer优势\",\"slug\":\"transformer优势\",\"link\":\"#transformer优势\",\"children\":[{\"level\":3,\"title\":\"1. 并行化\",\"slug\":\"_1-并行化\",\"link\":\"#_1-并行化\",\"children\":[]},{\"level\":3,\"title\":\"2. 长距离依赖\",\"slug\":\"_2-长距离依赖\",\"link\":\"#_2-长距离依赖\",\"children\":[]},{\"level\":3,\"title\":\"3. 可解释性\",\"slug\":\"_3-可解释性\",\"link\":\"#_3-可解释性\",\"children\":[]}]},{\"level\":2,\"title\":\"Transformer变体\",\"slug\":\"transformer变体\",\"link\":\"#transformer变体\",\"children\":[{\"level\":3,\"title\":\"1. BERT（Bidirectional Encoder Representations from Transformers）\",\"slug\":\"_1-bert-bidirectional-encoder-representations-from-transformers\",\"link\":\"#_1-bert-bidirectional-encoder-representations-from-transformers\",\"children\":[]},{\"level\":3,\"title\":\"2. GPT（Generative Pre-trained Transformer）\",\"slug\":\"_2-gpt-generative-pre-trained-transformer\",\"link\":\"#_2-gpt-generative-pre-trained-transformer\",\"children\":[]},{\"level\":3,\"title\":\"3. T5（Text-to-Text Transfer Transformer）\",\"slug\":\"_3-t5-text-to-text-transfer-transformer\",\"link\":\"#_3-t5-text-to-text-transfer-transformer\",\"children\":[]}]},{\"level\":2,\"title\":\"Transformer在计算机视觉中的应用\",\"slug\":\"transformer在计算机视觉中的应用\",\"link\":\"#transformer在计算机视觉中的应用\",\"children\":[{\"level\":3,\"title\":\"1. Vision Transformer（ViT）\",\"slug\":\"_1-vision-transformer-vit\",\"link\":\"#_1-vision-transformer-vit\",\"children\":[]},{\"level\":3,\"title\":\"2. Swin Transformer\",\"slug\":\"_2-swin-transformer\",\"link\":\"#_2-swin-transformer\",\"children\":[]}]},{\"level\":2,\"title\":\"优化与改进\",\"slug\":\"优化与改进\",\"link\":\"#优化与改进\",\"children\":[{\"level\":3,\"title\":\"1. Efficient Attention\",\"slug\":\"_1-efficient-attention\",\"link\":\"#_1-efficient-attention\",\"children\":[]},{\"level\":3,\"title\":\"2. 训练优化\",\"slug\":\"_2-训练优化\",\"link\":\"#_2-训练优化\",\"children\":[]}]},{\"level\":2,\"title\":\"实现要点\",\"slug\":\"实现要点\",\"link\":\"#实现要点\",\"children\":[{\"level\":3,\"title\":\"1. 初始化\",\"slug\":\"_1-初始化\",\"link\":\"#_1-初始化\",\"children\":[]},{\"level\":3,\"title\":\"2. 正则化\",\"slug\":\"_2-正则化\",\"link\":\"#_2-正则化\",\"children\":[]},{\"level\":3,\"title\":\"3. 批处理\",\"slug\":\"_3-批处理\",\"link\":\"#_3-批处理\",\"children\":[]}]},{\"level\":2,\"title\":\"应用场景\",\"slug\":\"应用场景\",\"link\":\"#应用场景\",\"children\":[{\"level\":3,\"title\":\"1. 自然语言处理\",\"slug\":\"_1-自然语言处理\",\"link\":\"#_1-自然语言处理\",\"children\":[]},{\"level\":3,\"title\":\"2. 语音处理\",\"slug\":\"_2-语音处理\",\"link\":\"#_2-语音处理\",\"children\":[]},{\"level\":3,\"title\":\"3. 计算机视觉\",\"slug\":\"_3-计算机视觉\",\"link\":\"#_3-计算机视觉\",\"children\":[]}]}],\"git\":{},\"filePathRelative\":\"bigdata/transformer-notes.md\",\"excerpt\":\"\\n<h2>Transformer简介</h2>\\n<p>Transformer是由Vaswani等人在2017年论文《Attention Is All You Need》中提出的一种全新的神经网络架构。它完全基于注意力机制，摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN）结构，在机器翻译等序列到序列任务中取得了显著的效果提升。</p>\\n<h2>Transformer核心思想</h2>\\n<h3>1. 注意力机制</h3>\\n<ul>\\n<li>解决长距离依赖问题</li>\\n<li>并行化计算，提高训练效率</li>\\n<li>更好地捕捉序列内元素间的关系</li>\\n</ul>\\n<h3>2. 编码器-解码器结构</h3>\"}")
export { comp, data }

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
