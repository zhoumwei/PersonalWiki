<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.20" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title>大模型微调(Fine-tuning)学习笔记 | PersonalWiki</title><meta name="description" content="My PersonalWiki Site">
    <link rel="preload" href="/assets/style-QXEKf4Y2.css" as="style"><link rel="stylesheet" href="/assets/style-QXEKf4Y2.css">
    <link rel="modulepreload" href="/assets/app-BvwJZ6kh.js"><link rel="modulepreload" href="/assets/fine-tuning.html-CjBB1ax_.js">
    <link rel="prefetch" href="/assets/index.html-CbTDEAts.js" as="script"><link rel="prefetch" href="/assets/get-started.html-CjD3fAAo.js" as="script"><link rel="prefetch" href="/assets/index.html-DSrufwo5.js" as="script"><link rel="prefetch" href="/assets/flink-notes.html-BpXJ8u78.js" as="script"><link rel="prefetch" href="/assets/hadoop-notes.html-C4UlBBUF.js" as="script"><link rel="prefetch" href="/assets/hbase-notes.html-CTid9EA9.js" as="script"><link rel="prefetch" href="/assets/hive-notes.html-BCTrtLqN.js" as="script"><link rel="prefetch" href="/assets/spark-notes.html-DjBDa7xt.js" as="script"><link rel="prefetch" href="/assets/time-series-data.html-Cpf5Qxjz.js" as="script"><link rel="prefetch" href="/assets/index.html-BWfX9Mq_.js" as="script"><link rel="prefetch" href="/assets/anomaly-detection.html-Bg2YjLXz.js" as="script"><link rel="prefetch" href="/assets/solution-overview.html-CetPEQ_x.js" as="script"><link rel="prefetch" href="/assets/index.html-D_AXRVv7.js" as="script"><link rel="prefetch" href="/assets/redis-notes.html-BjkbebZR.js" as="script"><link rel="prefetch" href="/assets/index.html-Df8gWSBm.js" as="script"><link rel="prefetch" href="/assets/mysql-notes.html-s5dCwXKf.js" as="script"><link rel="prefetch" href="/assets/index.html-eSWmtd0l.js" as="script"><link rel="prefetch" href="/assets/css-interview.html-DluBQhYO.js" as="script"><link rel="prefetch" href="/assets/html-interview.html-Cyy3AGVn.js" as="script"><link rel="prefetch" href="/assets/javascript-interview.html-CG7UI4-t.js" as="script"><link rel="prefetch" href="/assets/layout-methods.html-pL0_j4Ng.js" as="script"><link rel="prefetch" href="/assets/react-interview.html-CIa2Sn63.js" as="script"><link rel="prefetch" href="/assets/vue-interview.html-K5pEBcjG.js" as="script"><link rel="prefetch" href="/assets/index.html-TRTe0zxa.js" as="script"><link rel="prefetch" href="/assets/android-interview.html-ada45Htj.js" as="script"><link rel="prefetch" href="/assets/java-basic-interview.html-DjUXvpr3.js" as="script"><link rel="prefetch" href="/assets/jmm-notes.html-MDd7Ko4J.js" as="script"><link rel="prefetch" href="/assets/jvm-notes.html-DLuG0MRh.js" as="script"><link rel="prefetch" href="/assets/multithreading-interview.html-D5DrrSe3.js" as="script"><link rel="prefetch" href="/assets/multithreading-notes.html-DfVio_Fi.js" as="script"><link rel="prefetch" href="/assets/index.html-BnS01Ekr.js" as="script"><link rel="prefetch" href="/assets/llm-notes.html-QVyBkdng.js" as="script"><link rel="prefetch" href="/assets/prompt-engineering.html-DNhy9bB9.js" as="script"><link rel="prefetch" href="/assets/training-optimization.html-xAtwji5x.js" as="script"><link rel="prefetch" href="/assets/transformer-notes.html-CipMRP38.js" as="script"><link rel="prefetch" href="/assets/index.html-Cw2F1Agy.js" as="script"><link rel="prefetch" href="/assets/machine-learning.html-B-iGr0DC.js" as="script"><link rel="prefetch" href="/assets/index.html-Dg7P-8V_.js" as="script"><link rel="prefetch" href="/assets/kafka-notes.html-BetCbVDF.js" as="script"><link rel="prefetch" href="/assets/index.html-BKcXiGDb.js" as="script"><link rel="prefetch" href="/assets/nodejs-interview.html-B9dtC6T3.js" as="script"><link rel="prefetch" href="/assets/archive1.html-CU_-lS65.js" as="script"><link rel="prefetch" href="/assets/archive2.html-Dnyy_0hM.js" as="script"><link rel="prefetch" href="/assets/article1.html-An39HM8J.js" as="script"><link rel="prefetch" href="/assets/article10.html-B4wvKqrh.js" as="script"><link rel="prefetch" href="/assets/article11.html-BwLv2ZIG.js" as="script"><link rel="prefetch" href="/assets/article12.html-ey-3Numx.js" as="script"><link rel="prefetch" href="/assets/article2.html-B6IJZCZx.js" as="script"><link rel="prefetch" href="/assets/article3.html-WQYkFWda.js" as="script"><link rel="prefetch" href="/assets/article4.html-3reFRa1Y.js" as="script"><link rel="prefetch" href="/assets/article5.html-BAgHH7zk.js" as="script"><link rel="prefetch" href="/assets/article6.html-Ci6HTPLI.js" as="script"><link rel="prefetch" href="/assets/article7.html-CGCzq-HI.js" as="script"><link rel="prefetch" href="/assets/article8.html-DAf_qtAn.js" as="script"><link rel="prefetch" href="/assets/article9.html-BJ_PclXO.js" as="script"><link rel="prefetch" href="/assets/sticky.html-DzYDdEpa.js" as="script"><link rel="prefetch" href="/assets/sticky2.html-DxwKeFc6.js" as="script"><link rel="prefetch" href="/assets/index.html-SKjZhXJG.js" as="script"><link rel="prefetch" href="/assets/brew-tutorial.html-HINWWcf0.js" as="script"><link rel="prefetch" href="/assets/linux-notes.html-Bs6UdVSt.js" as="script"><link rel="prefetch" href="/assets/nvm-tutorial.html-CQ4cnbiJ.js" as="script"><link rel="prefetch" href="/assets/tencent-cloud-config.html-BAq6h3ta.js" as="script"><link rel="prefetch" href="/assets/404.html-Bocm2NQp.js" as="script"><link rel="prefetch" href="/assets/index.html-D7ucGqBj.js" as="script"><link rel="prefetch" href="/assets/index.html-Kt4soQse.js" as="script"><link rel="prefetch" href="/assets/index.html-DP6wngEx.js" as="script"><link rel="prefetch" href="/assets/index.html-CyRJUuAN.js" as="script"><link rel="prefetch" href="/assets/index.html-CSYpLAM6.js" as="script"><link rel="prefetch" href="/assets/index.html-DkIn-eRi.js" as="script"><link rel="prefetch" href="/assets/index.html-CFDck-Pn.js" as="script"><link rel="prefetch" href="/assets/index.html-BCj8RVxP.js" as="script"><link rel="prefetch" href="/assets/index.html-C6DXimEX.js" as="script"><link rel="prefetch" href="/assets/index.html-TPQAqjy9.js" as="script"><link rel="prefetch" href="/assets/index.html-Bdmct_gh.js" as="script"><link rel="prefetch" href="/assets/index.html-fHklgrv9.js" as="script"><link rel="prefetch" href="/assets/index.html-BKdawjmQ.js" as="script"><link rel="prefetch" href="/assets/index.html-Dam71-Kz.js" as="script"><link rel="prefetch" href="/assets/index.html-C4b-Z5oU.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="/logo.svg" alt="PersonalWiki"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">PersonalWiki</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/article/" aria-label="Article"><!--[--><!--[--><!--]--><!--]-->Article<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/aiops/" aria-label="AIOps"><!--[--><!--[--><!--]--><!--]-->AIOps<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/bigdata/" aria-label="BigData"><!--[--><!--[--><!--]--><!--]-->BigData<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/cache/" aria-label="Cache"><!--[--><!--[--><!--]--><!--]-->Cache<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/database/" aria-label="Database"><!--[--><!--[--><!--]--><!--]-->Database<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/java/" aria-label="Java"><!--[--><!--[--><!--]--><!--]-->Java<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/llm/" aria-label="LLM"><!--[--><!--[--><!--]--><!--]-->LLM<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/ml/" aria-label="ML"><!--[--><!--[--><!--]--><!--]-->ML<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/mq/" aria-label="MQ"><!--[--><!--[--><!--]--><!--]-->MQ<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/nodejs/" aria-label="Node.js"><!--[--><!--[--><!--]--><!--]-->Node.js<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/system/" aria-label="System"><!--[--><!--[--><!--]--><!--]-->System<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/category/" aria-label="Category"><!--[--><!--[--><!--]--><!--]-->Category<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/tag/" aria-label="Tag"><!--[--><!--[--><!--]--><!--]-->Tag<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/timeline/" aria-label="Timeline"><!--[--><!--[--><!--]--><!--]-->Timeline<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/article/" aria-label="Article"><!--[--><!--[--><!--]--><!--]-->Article<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/aiops/" aria-label="AIOps"><!--[--><!--[--><!--]--><!--]-->AIOps<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/bigdata/" aria-label="BigData"><!--[--><!--[--><!--]--><!--]-->BigData<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/cache/" aria-label="Cache"><!--[--><!--[--><!--]--><!--]-->Cache<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/database/" aria-label="Database"><!--[--><!--[--><!--]--><!--]-->Database<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/java/" aria-label="Java"><!--[--><!--[--><!--]--><!--]-->Java<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/llm/" aria-label="LLM"><!--[--><!--[--><!--]--><!--]-->LLM<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/ml/" aria-label="ML"><!--[--><!--[--><!--]--><!--]-->ML<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/mq/" aria-label="MQ"><!--[--><!--[--><!--]--><!--]-->MQ<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/nodejs/" aria-label="Node.js"><!--[--><!--[--><!--]--><!--]-->Node.js<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/system/" aria-label="System"><!--[--><!--[--><!--]--><!--]-->System<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/category/" aria-label="Category"><!--[--><!--[--><!--]--><!--]-->Category<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/tag/" aria-label="Tag"><!--[--><!--[--><!--]--><!--]-->Tag<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/timeline/" aria-label="Timeline"><!--[--><!--[--><!--]--><!--]-->Timeline<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading active">大语言模型 <!----></p><ul class="vp-sidebar-children" style=""><!--[--><li><a class="route-link route-link-active auto-link vp-sidebar-item" href="/llm/" aria-label="大语言模型(LLM)学习笔记"><!--[--><!--[--><!--]--><!--]-->大语言模型(LLM)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/llm-notes.html" aria-label="大模型(LLM)学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型(LLM)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/transformer-notes.html" aria-label="Transformer架构学习笔记"><!--[--><!--[--><!--]--><!--]-->Transformer架构学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/prompt-engineering.html" aria-label="提示工程(Prompt Engineering)学习笔记"><!--[--><!--[--><!--]--><!--]-->提示工程(Prompt Engineering)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link route-link-active auto-link vp-sidebar-item active" href="/llm/fine-tuning.html" aria-label="大模型微调(Fine-tuning)学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型微调(Fine-tuning)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/training-optimization.html" aria-label="大模型训练与优化学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型训练与优化学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div><h1 id="大模型微调-fine-tuning-学习笔记" tabindex="-1"><a class="header-anchor" href="#大模型微调-fine-tuning-学习笔记"><span>大模型微调(Fine-tuning)学习笔记</span></a></h1><h2 id="什么是大模型微调" tabindex="-1"><a class="header-anchor" href="#什么是大模型微调"><span>什么是大模型微调</span></a></h2><p>大模型微调是指在预训练的大语言模型基础上，使用特定领域或任务的数据进行进一步训练，以使模型更好地适应特定应用场景的过程。相比于从头开始训练模型，微调能够显著减少所需的计算资源和时间。</p><h2 id="微调与提示工程的比较" tabindex="-1"><a class="header-anchor" href="#微调与提示工程的比较"><span>微调与提示工程的比较</span></a></h2><table><thead><tr><th>方面</th><th>提示工程</th><th>微调</th></tr></thead><tbody><tr><td>成本</td><td>低（无需训练）</td><td>高（需要计算资源）</td></tr><tr><td>时间</td><td>即时</td><td>需要训练时间</td></tr><tr><td>效果</td><td>依赖于提示质量</td><td>通常效果更好</td></tr><tr><td>通用性</td><td>适用于任何模型</td><td>需要针对具体模型</td></tr><tr><td>可移植性</td><td>高</td><td>低（需保存模型）</td></tr></tbody></table><h2 id="微调的基本原理" tabindex="-1"><a class="header-anchor" href="#微调的基本原理"><span>微调的基本原理</span></a></h2><h3 id="_1-迁移学习" tabindex="-1"><a class="header-anchor" href="#_1-迁移学习"><span>1. 迁移学习</span></a></h3><ul><li>利用预训练模型学到的通用知识</li><li>在特定任务上进行适应性调整</li></ul><h3 id="_2-参数更新" tabindex="-1"><a class="header-anchor" href="#_2-参数更新"><span>2. 参数更新</span></a></h3><ul><li>冻结部分层，只训练顶层</li><li>或者全部参数一起训练</li></ul><h3 id="_3-学习率设置" tabindex="-1"><a class="header-anchor" href="#_3-学习率设置"><span>3. 学习率设置</span></a></h3><ul><li>通常使用较小的学习率</li><li>避免破坏预训练模型的知识</li></ul><h2 id="微调方法" tabindex="-1"><a class="header-anchor" href="#微调方法"><span>微调方法</span></a></h2><h3 id="_1-全参数微调-full-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_1-全参数微调-full-fine-tuning"><span>1. 全参数微调(Full Fine-tuning)</span></a></h3><p>更新模型的所有参数，适用于：</p><ul><li>有足够的计算资源</li><li>有大量高质量的训练数据</li><li>需要最大化的性能提升</li></ul><p>优点：</p><ul><li>性能提升最为显著</li><li>模型适应性最强</li></ul><p>缺点：</p><ul><li>计算成本高</li><li>容易过拟合</li><li>存储开销大</li></ul><h3 id="_2-部分参数微调" tabindex="-1"><a class="header-anchor" href="#_2-部分参数微调"><span>2. 部分参数微调</span></a></h3><p>只更新模型的部分参数：</p><h4 id="a-layer-freezing" tabindex="-1"><a class="header-anchor" href="#a-layer-freezing"><span>a. Layer Freezing</span></a></h4><p>冻结底层参数，只训练顶层：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line"># 示例：冻结前6层</span>
<span class="line">for param in model.parameters():</span>
<span class="line">    param.requires_grad = False</span>
<span class="line">    </span>
<span class="line"># 只训练最后几层</span>
<span class="line">for param in model.layers[-2:].parameters():</span>
<span class="line">    param.requires_grad = True</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="b-bias-only-fine-tuning" tabindex="-1"><a class="header-anchor" href="#b-bias-only-fine-tuning"><span>b. Bias-only Fine-tuning</span></a></h4><p>只训练偏置参数，保持权重不变。</p><h3 id="_3-适配器微调-adapter-fine-tuning" tabindex="-1"><a class="header-anchor" href="#_3-适配器微调-adapter-fine-tuning"><span>3. 适配器微调(Adapter Fine-tuning)</span></a></h3><p>在模型的每一层中插入小型神经网络模块(适配器)：</p><p>优点：</p><ul><li>参数效率高</li><li>易于部署多个适配器</li><li>不改变原模型结构</li></ul><p>缺点：</p><ul><li>训练和推理时会有额外延迟</li><li>性能可能略低于全参数微调</li></ul><h3 id="_4-lora-low-rank-adaptation" tabindex="-1"><a class="header-anchor" href="#_4-lora-low-rank-adaptation"><span>4. LoRA (Low-Rank Adaptation)</span></a></h3><p>将权重更新矩阵分解为低秩矩阵的乘积：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">ΔW = A × B</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>其中A和B是低秩矩阵，参数量远小于原始权重矩阵。</p><p>优点：</p><ul><li>极大地减少了可训练参数</li><li>训练完成后可以合并到原模型中</li><li>易于切换不同的适配模块</li></ul><p>缺点：</p><ul><li>需要修改模型结构</li><li>对某些任务效果可能不如全参数微调</li></ul><h3 id="_5-prompt-tuning" tabindex="-1"><a class="header-anchor" href="#_5-prompt-tuning"><span>5. Prompt Tuning</span></a></h3><p>只训练输入提示部分的参数向量，而不是模型本身：</p><p>优点：</p><ul><li>参数效率极高</li><li>保持模型不变</li><li>易于部署</li></ul><p>缺点：</p><ul><li>对于复杂任务效果有限</li><li>需要较长的提示序列</li></ul><h3 id="_6-prefix-tuning" tabindex="-1"><a class="header-anchor" href="#_6-prefix-tuning"><span>6. Prefix Tuning</span></a></h3><p>在每一层的输入前添加可学习的前缀向量：</p><p>优点：</p><ul><li>比Prompt Tuning更强大</li><li>逐层控制模型行为</li></ul><p>缺点：</p><ul><li>实现相对复杂</li><li>需要调整模型输入</li></ul><h2 id="微调步骤" tabindex="-1"><a class="header-anchor" href="#微调步骤"><span>微调步骤</span></a></h2><h3 id="_1-数据准备" tabindex="-1"><a class="header-anchor" href="#_1-数据准备"><span>1. 数据准备</span></a></h3><ul><li>收集和清洗训练数据</li><li>格式化为模型输入格式</li><li>划分训练集、验证集和测试集</li></ul><h3 id="_2-模型选择" tabindex="-1"><a class="header-anchor" href="#_2-模型选择"><span>2. 模型选择</span></a></h3><ul><li>选择合适的预训练模型</li><li>考虑模型大小和计算资源</li><li>考虑任务特点和数据量</li></ul><h3 id="_3-微调设置" tabindex="-1"><a class="header-anchor" href="#_3-微调设置"><span>3. 微调设置</span></a></h3><ul><li>设置学习率和优化器</li><li>选择批次大小和训练轮数</li><li>设置早停机制防止过拟合</li></ul><h3 id="_4-训练过程" tabindex="-1"><a class="header-anchor" href="#_4-训练过程"><span>4. 训练过程</span></a></h3><ul><li>监控训练损失和验证指标</li><li>调整超参数</li><li>保存最佳模型</li></ul><h3 id="_5-评估和部署" tabindex="-1"><a class="header-anchor" href="#_5-评估和部署"><span>5. 评估和部署</span></a></h3><ul><li>在测试集上评估性能</li><li>部署到生产环境</li><li>持续监控和优化</li></ul><h2 id="微调关键技术" tabindex="-1"><a class="header-anchor" href="#微调关键技术"><span>微调关键技术</span></a></h2><h3 id="_1-学习率调度" tabindex="-1"><a class="header-anchor" href="#_1-学习率调度"><span>1. 学习率调度</span></a></h3><ul><li>预热(warmup)策略</li><li>余弦退火(cosine annealing)</li><li>分层学习率(layer-wise learning rate)</li></ul><h3 id="_2-正则化技术" tabindex="-1"><a class="header-anchor" href="#_2-正则化技术"><span>2. 正则化技术</span></a></h3><ul><li>Dropout防止过拟合</li><li>权重衰减(weight decay)</li><li>梯度裁剪(gradient clipping)</li></ul><h3 id="_3-批量大小选择" tabindex="-1"><a class="header-anchor" href="#_3-批量大小选择"><span>3. 批量大小选择</span></a></h3><ul><li>根据GPU内存确定</li><li>影响训练稳定性和收敛速度</li><li>可使用梯度累积模拟大批量训练</li></ul><h3 id="_4-混合精度训练" tabindex="-1"><a class="header-anchor" href="#_4-混合精度训练"><span>4. 混合精度训练</span></a></h3><ul><li>使用FP16减少内存占用</li><li>加速训练过程</li><li>保持模型精度</li></ul><h2 id="微调最佳实践" tabindex="-1"><a class="header-anchor" href="#微调最佳实践"><span>微调最佳实践</span></a></h2><h3 id="_1-数据质量优于数量" tabindex="-1"><a class="header-anchor" href="#_1-数据质量优于数量"><span>1. 数据质量优于数量</span></a></h3><ul><li>高质量的小数据集往往比低质量的大数据集效果更好</li><li>注重数据的多样性和代表性</li></ul><h3 id="_2-渐进式微调" tabindex="-1"><a class="header-anchor" href="#_2-渐进式微调"><span>2. 渐进式微调</span></a></h3><ul><li>先使用较小学习率进行初步微调</li><li>再使用更大学习率进行精细调整</li></ul><h3 id="_3-多任务学习" tabindex="-1"><a class="header-anchor" href="#_3-多任务学习"><span>3. 多任务学习</span></a></h3><ul><li>在相关任务上联合微调</li><li>提高模型的泛化能力</li></ul><h3 id="_4-正则化策略" tabindex="-1"><a class="header-anchor" href="#_4-正则化策略"><span>4. 正则化策略</span></a></h3><ul><li>合理使用Dropout</li><li>设置适当的学习率衰减</li><li>使用早停机制</li></ul><h3 id="_5-监控和调试" tabindex="-1"><a class="header-anchor" href="#_5-监控和调试"><span>5. 监控和调试</span></a></h3><ul><li>实时监控训练曲线</li><li>使用可视化工具分析注意力权重</li><li>定期评估验证集性能</li></ul><h2 id="微调评估指标" tabindex="-1"><a class="header-anchor" href="#微调评估指标"><span>微调评估指标</span></a></h2><h3 id="_1-任务特定指标" tabindex="-1"><a class="header-anchor" href="#_1-任务特定指标"><span>1. 任务特定指标</span></a></h3><ul><li>分类任务：准确率、F1分数、AUC</li><li>生成任务：BLEU、ROUGE、METEOR</li><li>问答任务：EM(Exact Match)、F1</li></ul><h3 id="_2-通用指标" tabindex="-1"><a class="header-anchor" href="#_2-通用指标"><span>2. 通用指标</span></a></h3><ul><li>困惑度(Perplexity)</li><li>生成多样性</li><li>事实一致性</li></ul><h3 id="_3-效率指标" tabindex="-1"><a class="header-anchor" href="#_3-效率指标"><span>3. 效率指标</span></a></h3><ul><li>训练时间</li><li>推理延迟</li><li>模型大小</li></ul><h2 id="微调工具和框架" tabindex="-1"><a class="header-anchor" href="#微调工具和框架"><span>微调工具和框架</span></a></h2><h3 id="_1-hugging-face-transformers" tabindex="-1"><a class="header-anchor" href="#_1-hugging-face-transformers"><span>1. Hugging Face Transformers</span></a></h3><ul><li>最流行的NLP模型库</li><li>支持众多预训练模型</li><li>提供完整的微调工具链</li></ul><h3 id="_2-pytorch-lightning" tabindex="-1"><a class="header-anchor" href="#_2-pytorch-lightning"><span>2. PyTorch Lightning</span></a></h3><ul><li>简化PyTorch训练流程</li><li>内置最佳实践</li><li>支持分布式训练</li></ul><h3 id="_3-deepspeed" tabindex="-1"><a class="header-anchor" href="#_3-deepspeed"><span>3. DeepSpeed</span></a></h3><ul><li>微软的深度学习优化库</li><li>支持大规模模型训练</li><li>提供ZeRO优化器</li></ul><h3 id="_4-accelerate" tabindex="-1"><a class="header-anchor" href="#_4-accelerate"><span>4. Accelerate</span></a></h3><ul><li>Hugging Face的训练加速库</li><li>简化多GPU和TPU训练</li><li>自动处理设备放置</li></ul><h2 id="常见问题与解决方案" tabindex="-1"><a class="header-anchor" href="#常见问题与解决方案"><span>常见问题与解决方案</span></a></h2><h3 id="_1-内存不足" tabindex="-1"><a class="header-anchor" href="#_1-内存不足"><span>1. 内存不足</span></a></h3><p>解决方案：</p><ul><li>使用梯度检查点</li><li>减少批次大小</li><li>使用混合精度训练</li><li>应用模型并行</li></ul><h3 id="_2-过拟合" tabindex="-1"><a class="header-anchor" href="#_2-过拟合"><span>2. 过拟合</span></a></h3><p>解决方案：</p><ul><li>增加正则化</li><li>使用早停机制</li><li>增加训练数据</li><li>简化模型结构</li></ul><h3 id="_3-训练不稳定" tabindex="-1"><a class="header-anchor" href="#_3-训练不稳定"><span>3. 训练不稳定</span></a></h3><p>解决方案：</p><ul><li>降低学习率</li><li>使用学习率预热</li><li>检查数据质量</li><li>调整批次大小</li></ul><h3 id="_4-性能不佳" tabindex="-1"><a class="header-anchor" href="#_4-性能不佳"><span>4. 性能不佳</span></a></h3><p>解决方案：</p><ul><li>检查数据标注质量</li><li>调整超参数</li><li>尝试不同的微调方法</li><li>增加训练轮数</li></ul><h2 id="微调发展趋势" tabindex="-1"><a class="header-anchor" href="#微调发展趋势"><span>微调发展趋势</span></a></h2><h3 id="_1-参数高效微调" tabindex="-1"><a class="header-anchor" href="#_1-参数高效微调"><span>1. 参数高效微调</span></a></h3><ul><li>更多的参数高效方法涌现</li><li>在保持性能的同时减少参数量</li></ul><h3 id="_2-自适应微调" tabindex="-1"><a class="header-anchor" href="#_2-自适应微调"><span>2. 自适应微调</span></a></h3><ul><li>根据任务特点自动选择微调策略</li><li>动态调整微调参数</li></ul><h3 id="_3-多模态微调" tabindex="-1"><a class="header-anchor" href="#_3-多模态微调"><span>3. 多模态微调</span></a></h3><ul><li>文本、图像、音频等多模态联合微调</li><li>跨模态知识迁移</li></ul><h3 id="_4-联邦微调" tabindex="-1"><a class="header-anchor" href="#_4-联邦微调"><span>4. 联邦微调</span></a></h3><ul><li>在保护数据隐私的前提下进行微调</li><li>分布式协作训练</li></ul><p>大模型微调是一项复杂但非常有价值的技术，它能够让通用的大语言模型更好地适应特定的应用场景，从而在实际项目中发挥更大的作用。</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated:: </span><time class="meta-item-info" datetime="2025-12-07T07:07:49.000Z" data-allow-mismatch>12/7/25, 7:07 AM</time></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: 1127699551@qq.com">mingwzh</span><!----><!--]--><!--]--></span></div></div></footer><nav class="vp-page-nav" aria-label="page navigation"><a class="route-link auto-link prev" href="/llm/prompt-engineering.html" aria-label="提示工程(Prompt Engineering)学习笔记"><!--[--><div class="hint"><span class="arrow left"></span> Prev</div><div class="link"><span class="external-link">提示工程(Prompt Engineering)学习笔记</span></div><!--]--></a><a class="route-link auto-link next" href="/llm/training-optimization.html" aria-label="大模型训练与优化学习笔记"><!--[--><div class="hint">Next <span class="arrow right"></span></div><div class="link"><span class="external-link">大模型训练与优化学习笔记</span></div><!--]--></a></nav><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BvwJZ6kh.js" defer></script>
  </body>
</html>
