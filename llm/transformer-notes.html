<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.20" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <title>Transformer架构学习笔记 | PersonalWiki</title><meta name="description" content="My PersonalWiki Site">
    <link rel="preload" href="/assets/style-QXEKf4Y2.css" as="style"><link rel="stylesheet" href="/assets/style-QXEKf4Y2.css">
    <link rel="modulepreload" href="/assets/app-BvwJZ6kh.js"><link rel="modulepreload" href="/assets/transformer-notes.html-CipMRP38.js">
    <link rel="prefetch" href="/assets/index.html-CbTDEAts.js" as="script"><link rel="prefetch" href="/assets/get-started.html-CjD3fAAo.js" as="script"><link rel="prefetch" href="/assets/index.html-DSrufwo5.js" as="script"><link rel="prefetch" href="/assets/flink-notes.html-BpXJ8u78.js" as="script"><link rel="prefetch" href="/assets/hadoop-notes.html-C4UlBBUF.js" as="script"><link rel="prefetch" href="/assets/hbase-notes.html-CTid9EA9.js" as="script"><link rel="prefetch" href="/assets/hive-notes.html-BCTrtLqN.js" as="script"><link rel="prefetch" href="/assets/spark-notes.html-DjBDa7xt.js" as="script"><link rel="prefetch" href="/assets/time-series-data.html-Cpf5Qxjz.js" as="script"><link rel="prefetch" href="/assets/index.html-BWfX9Mq_.js" as="script"><link rel="prefetch" href="/assets/anomaly-detection.html-Bg2YjLXz.js" as="script"><link rel="prefetch" href="/assets/solution-overview.html-CetPEQ_x.js" as="script"><link rel="prefetch" href="/assets/index.html-D_AXRVv7.js" as="script"><link rel="prefetch" href="/assets/redis-notes.html-BjkbebZR.js" as="script"><link rel="prefetch" href="/assets/index.html-Df8gWSBm.js" as="script"><link rel="prefetch" href="/assets/mysql-notes.html-s5dCwXKf.js" as="script"><link rel="prefetch" href="/assets/index.html-eSWmtd0l.js" as="script"><link rel="prefetch" href="/assets/css-interview.html-DluBQhYO.js" as="script"><link rel="prefetch" href="/assets/html-interview.html-Cyy3AGVn.js" as="script"><link rel="prefetch" href="/assets/javascript-interview.html-CG7UI4-t.js" as="script"><link rel="prefetch" href="/assets/layout-methods.html-pL0_j4Ng.js" as="script"><link rel="prefetch" href="/assets/react-interview.html-CIa2Sn63.js" as="script"><link rel="prefetch" href="/assets/vue-interview.html-K5pEBcjG.js" as="script"><link rel="prefetch" href="/assets/index.html-TRTe0zxa.js" as="script"><link rel="prefetch" href="/assets/android-interview.html-ada45Htj.js" as="script"><link rel="prefetch" href="/assets/java-basic-interview.html-DjUXvpr3.js" as="script"><link rel="prefetch" href="/assets/jmm-notes.html-MDd7Ko4J.js" as="script"><link rel="prefetch" href="/assets/jvm-notes.html-DLuG0MRh.js" as="script"><link rel="prefetch" href="/assets/multithreading-interview.html-D5DrrSe3.js" as="script"><link rel="prefetch" href="/assets/multithreading-notes.html-DfVio_Fi.js" as="script"><link rel="prefetch" href="/assets/index.html-BnS01Ekr.js" as="script"><link rel="prefetch" href="/assets/fine-tuning.html-CjBB1ax_.js" as="script"><link rel="prefetch" href="/assets/llm-notes.html-QVyBkdng.js" as="script"><link rel="prefetch" href="/assets/prompt-engineering.html-DNhy9bB9.js" as="script"><link rel="prefetch" href="/assets/training-optimization.html-xAtwji5x.js" as="script"><link rel="prefetch" href="/assets/index.html-Cw2F1Agy.js" as="script"><link rel="prefetch" href="/assets/machine-learning.html-B-iGr0DC.js" as="script"><link rel="prefetch" href="/assets/index.html-Dg7P-8V_.js" as="script"><link rel="prefetch" href="/assets/kafka-notes.html-BetCbVDF.js" as="script"><link rel="prefetch" href="/assets/index.html-BKcXiGDb.js" as="script"><link rel="prefetch" href="/assets/nodejs-interview.html-B9dtC6T3.js" as="script"><link rel="prefetch" href="/assets/archive1.html-CU_-lS65.js" as="script"><link rel="prefetch" href="/assets/archive2.html-Dnyy_0hM.js" as="script"><link rel="prefetch" href="/assets/article1.html-An39HM8J.js" as="script"><link rel="prefetch" href="/assets/article10.html-B4wvKqrh.js" as="script"><link rel="prefetch" href="/assets/article11.html-BwLv2ZIG.js" as="script"><link rel="prefetch" href="/assets/article12.html-ey-3Numx.js" as="script"><link rel="prefetch" href="/assets/article2.html-B6IJZCZx.js" as="script"><link rel="prefetch" href="/assets/article3.html-WQYkFWda.js" as="script"><link rel="prefetch" href="/assets/article4.html-3reFRa1Y.js" as="script"><link rel="prefetch" href="/assets/article5.html-BAgHH7zk.js" as="script"><link rel="prefetch" href="/assets/article6.html-Ci6HTPLI.js" as="script"><link rel="prefetch" href="/assets/article7.html-CGCzq-HI.js" as="script"><link rel="prefetch" href="/assets/article8.html-DAf_qtAn.js" as="script"><link rel="prefetch" href="/assets/article9.html-BJ_PclXO.js" as="script"><link rel="prefetch" href="/assets/sticky.html-DzYDdEpa.js" as="script"><link rel="prefetch" href="/assets/sticky2.html-DxwKeFc6.js" as="script"><link rel="prefetch" href="/assets/index.html-SKjZhXJG.js" as="script"><link rel="prefetch" href="/assets/brew-tutorial.html-HINWWcf0.js" as="script"><link rel="prefetch" href="/assets/linux-notes.html-Bs6UdVSt.js" as="script"><link rel="prefetch" href="/assets/nvm-tutorial.html-CQ4cnbiJ.js" as="script"><link rel="prefetch" href="/assets/tencent-cloud-config.html-BAq6h3ta.js" as="script"><link rel="prefetch" href="/assets/404.html-Bocm2NQp.js" as="script"><link rel="prefetch" href="/assets/index.html-D7ucGqBj.js" as="script"><link rel="prefetch" href="/assets/index.html-Kt4soQse.js" as="script"><link rel="prefetch" href="/assets/index.html-DP6wngEx.js" as="script"><link rel="prefetch" href="/assets/index.html-CyRJUuAN.js" as="script"><link rel="prefetch" href="/assets/index.html-CSYpLAM6.js" as="script"><link rel="prefetch" href="/assets/index.html-DkIn-eRi.js" as="script"><link rel="prefetch" href="/assets/index.html-CFDck-Pn.js" as="script"><link rel="prefetch" href="/assets/index.html-BCj8RVxP.js" as="script"><link rel="prefetch" href="/assets/index.html-C6DXimEX.js" as="script"><link rel="prefetch" href="/assets/index.html-TPQAqjy9.js" as="script"><link rel="prefetch" href="/assets/index.html-Bdmct_gh.js" as="script"><link rel="prefetch" href="/assets/index.html-fHklgrv9.js" as="script"><link rel="prefetch" href="/assets/index.html-BKdawjmQ.js" as="script"><link rel="prefetch" href="/assets/index.html-Dam71-Kz.js" as="script"><link rel="prefetch" href="/assets/index.html-C4b-Z5oU.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="/logo.svg" alt="PersonalWiki"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">PersonalWiki</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/article/" aria-label="Article"><!--[--><!--[--><!--]--><!--]-->Article<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/aiops/" aria-label="AIOps"><!--[--><!--[--><!--]--><!--]-->AIOps<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/bigdata/" aria-label="BigData"><!--[--><!--[--><!--]--><!--]-->BigData<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/cache/" aria-label="Cache"><!--[--><!--[--><!--]--><!--]-->Cache<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/database/" aria-label="Database"><!--[--><!--[--><!--]--><!--]-->Database<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/java/" aria-label="Java"><!--[--><!--[--><!--]--><!--]-->Java<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/llm/" aria-label="LLM"><!--[--><!--[--><!--]--><!--]-->LLM<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/ml/" aria-label="ML"><!--[--><!--[--><!--]--><!--]-->ML<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/mq/" aria-label="MQ"><!--[--><!--[--><!--]--><!--]-->MQ<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/nodejs/" aria-label="Node.js"><!--[--><!--[--><!--]--><!--]-->Node.js<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/system/" aria-label="System"><!--[--><!--[--><!--]--><!--]-->System<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/category/" aria-label="Category"><!--[--><!--[--><!--]--><!--]-->Category<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/tag/" aria-label="Tag"><!--[--><!--[--><!--]--><!--]-->Tag<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/timeline/" aria-label="Timeline"><!--[--><!--[--><!--]--><!--]-->Timeline<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/article/" aria-label="Article"><!--[--><!--[--><!--]--><!--]-->Article<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/aiops/" aria-label="AIOps"><!--[--><!--[--><!--]--><!--]-->AIOps<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/bigdata/" aria-label="BigData"><!--[--><!--[--><!--]--><!--]-->BigData<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/cache/" aria-label="Cache"><!--[--><!--[--><!--]--><!--]-->Cache<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/database/" aria-label="Database"><!--[--><!--[--><!--]--><!--]-->Database<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/java/" aria-label="Java"><!--[--><!--[--><!--]--><!--]-->Java<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link route-link-active auto-link" href="/llm/" aria-label="LLM"><!--[--><!--[--><!--]--><!--]-->LLM<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/ml/" aria-label="ML"><!--[--><!--[--><!--]--><!--]-->ML<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/mq/" aria-label="MQ"><!--[--><!--[--><!--]--><!--]-->MQ<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/nodejs/" aria-label="Node.js"><!--[--><!--[--><!--]--><!--]-->Node.js<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/system/" aria-label="System"><!--[--><!--[--><!--]--><!--]-->System<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/category/" aria-label="Category"><!--[--><!--[--><!--]--><!--]-->Category<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/tag/" aria-label="Tag"><!--[--><!--[--><!--]--><!--]-->Tag<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="route-link auto-link" href="/timeline/" aria-label="Timeline"><!--[--><!--[--><!--]--><!--]-->Timeline<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading active">大语言模型 <!----></p><ul class="vp-sidebar-children" style=""><!--[--><li><a class="route-link route-link-active auto-link vp-sidebar-item" href="/llm/" aria-label="大语言模型(LLM)学习笔记"><!--[--><!--[--><!--]--><!--]-->大语言模型(LLM)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/llm-notes.html" aria-label="大模型(LLM)学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型(LLM)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link route-link-active auto-link vp-sidebar-item active" href="/llm/transformer-notes.html" aria-label="Transformer架构学习笔记"><!--[--><!--[--><!--]--><!--]-->Transformer架构学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/prompt-engineering.html" aria-label="提示工程(Prompt Engineering)学习笔记"><!--[--><!--[--><!--]--><!--]-->提示工程(Prompt Engineering)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/fine-tuning.html" aria-label="大模型微调(Fine-tuning)学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型微调(Fine-tuning)学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><li><a class="route-link auto-link vp-sidebar-item" href="/llm/training-optimization.html" aria-label="大模型训练与优化学习笔记"><!--[--><!--[--><!--]--><!--]-->大模型训练与优化学习笔记<!--[--><!--[--><!--]--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div><h1 id="transformer架构学习笔记" tabindex="-1"><a class="header-anchor" href="#transformer架构学习笔记"><span>Transformer架构学习笔记</span></a></h1><h2 id="transformer简介" tabindex="-1"><a class="header-anchor" href="#transformer简介"><span>Transformer简介</span></a></h2><p>Transformer是由Vaswani等人在2017年论文《Attention Is All You Need》中提出的一种全新的神经网络架构。它完全基于注意力机制，摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN）结构，在机器翻译等序列到序列任务中取得了显著的效果提升。</p><h2 id="transformer核心思想" tabindex="-1"><a class="header-anchor" href="#transformer核心思想"><span>Transformer核心思想</span></a></h2><h3 id="_1-注意力机制" tabindex="-1"><a class="header-anchor" href="#_1-注意力机制"><span>1. 注意力机制</span></a></h3><ul><li>解决长距离依赖问题</li><li>并行化计算，提高训练效率</li><li>更好地捕捉序列内元素间的关系</li></ul><h3 id="_2-编码器-解码器结构" tabindex="-1"><a class="header-anchor" href="#_2-编码器-解码器结构"><span>2. 编码器-解码器结构</span></a></h3><ul><li>编码器：将输入序列转换为连续表示</li><li>解码器：基于编码器输出和前面的输出生成目标序列</li></ul><h2 id="transformer整体架构" tabindex="-1"><a class="header-anchor" href="#transformer整体架构"><span>Transformer整体架构</span></a></h2><h3 id="_1-编码器-encoder" tabindex="-1"><a class="header-anchor" href="#_1-编码器-encoder"><span>1. 编码器（Encoder）</span></a></h3><ul><li>由6个相同的层堆叠而成</li><li>每层包含两个子层： <ul><li>多头自注意力机制（Multi-Head Self-Attention）</li><li>位置前馈网络（Position-wise Feed-Forward Networks）</li></ul></li></ul><h3 id="_2-解码器-decoder" tabindex="-1"><a class="header-anchor" href="#_2-解码器-decoder"><span>2. 解码器（Decoder）</span></a></h3><ul><li>由6个相同的层堆叠而成</li><li>每层包含三个子层： <ul><li>多头自注意力机制（Masked Multi-Head Self-Attention）</li><li>编码器-解码器注意力机制（Multi-Head Attention）</li><li>位置前馈网络（Position-wise Feed-Forward Networks）</li></ul></li></ul><h2 id="核心组件详解" tabindex="-1"><a class="header-anchor" href="#核心组件详解"><span>核心组件详解</span></a></h2><h3 id="_1-自注意力机制-self-attention" tabindex="-1"><a class="header-anchor" href="#_1-自注意力机制-self-attention"><span>1. 自注意力机制（Self-Attention）</span></a></h3><p>自注意力机制允许模型在处理序列时关注序列的不同位置，从而更好地理解上下文。</p><h4 id="计算过程" tabindex="-1"><a class="header-anchor" href="#计算过程"><span>计算过程：</span></a></h4><ol><li>对输入序列中的每个元素计算Query、Key、Value向量</li><li>计算注意力分数：Score = Q × K^T / √d_k</li><li>应用Softmax函数得到注意力权重</li><li>加权求和Value向量得到输出</li></ol><h4 id="公式表达" tabindex="-1"><a class="header-anchor" href="#公式表达"><span>公式表达：</span></a></h4><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">Attention(Q, K, V) = softmax(QK^T / √d_k)V</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>其中d_k是Key向量的维度。</p><h3 id="_2-多头注意力-multi-head-attention" tabindex="-1"><a class="header-anchor" href="#_2-多头注意力-multi-head-attention"><span>2. 多头注意力（Multi-Head Attention）</span></a></h3><p>通过并行计算多个注意力头，可以让模型关注不同子空间的信息。</p><h4 id="结构" tabindex="-1"><a class="header-anchor" href="#结构"><span>结构：</span></a></h4><ul><li>将Q、K、V分别投影到h个不同的子空间</li><li>在每个子空间中并行计算注意力</li><li>将所有头的结果拼接并通过线性变换得到最终输出</li></ul><h4 id="公式表达-1" tabindex="-1"><a class="header-anchor" href="#公式表达-1"><span>公式表达：</span></a></h4><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O</span>
<span class="line">其中 head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-位置编码-positional-encoding" tabindex="-1"><a class="header-anchor" href="#_3-位置编码-positional-encoding"><span>3. 位置编码（Positional Encoding）</span></a></h3><p>由于Transformer不包含循环或卷积结构，需要显式添加位置信息。</p><h4 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法：</span></a></h4><ul><li>使用正弦和余弦函数生成位置编码</li><li>对于位置pos和维度i： <ul><li>PE_(pos,2i) = sin(pos/10000^(2i/d_model))</li><li>PE_(pos,2i+1) = cos(pos/10000^(2i/d_model))</li></ul></li></ul><h3 id="_4-位置前馈网络-position-wise-feed-forward-networks" tabindex="-1"><a class="header-anchor" href="#_4-位置前馈网络-position-wise-feed-forward-networks"><span>4. 位置前馈网络（Position-wise Feed-Forward Networks）</span></a></h3><p>每个位置的数据通过相同的全连接网络进行处理。</p><h4 id="结构-1" tabindex="-1"><a class="header-anchor" href="#结构-1"><span>结构：</span></a></h4><ul><li>两层线性变换，中间使用ReLU激活函数</li><li>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2</li></ul><h2 id="残差连接与层归一化" tabindex="-1"><a class="header-anchor" href="#残差连接与层归一化"><span>残差连接与层归一化</span></a></h2><h3 id="_1-残差连接-residual-connection" tabindex="-1"><a class="header-anchor" href="#_1-残差连接-residual-connection"><span>1. 残差连接（Residual Connection）</span></a></h3><ul><li>缓解深层网络的梯度消失问题</li><li>加快训练收敛速度</li></ul><h3 id="_2-层归一化-layer-normalization" tabindex="-1"><a class="header-anchor" href="#_2-层归一化-layer-normalization"><span>2. 层归一化（Layer Normalization）</span></a></h3><ul><li>在每个子层输出后应用</li><li>稳定训练过程</li></ul><h3 id="_3-结构" tabindex="-1"><a class="header-anchor" href="#_3-结构"><span>3. 结构</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">LayerNorm(x + Sublayer(x))</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="解码器特殊设计" tabindex="-1"><a class="header-anchor" href="#解码器特殊设计"><span>解码器特殊设计</span></a></h2><h3 id="_1-掩码自注意力-masked-self-attention" tabindex="-1"><a class="header-anchor" href="#_1-掩码自注意力-masked-self-attention"><span>1. 掩码自注意力（Masked Self-Attention）</span></a></h3><ul><li>防止位置关注后续位置</li><li>保证预测时只能使用已知信息</li></ul><h3 id="_2-编码器-解码器注意力" tabindex="-1"><a class="header-anchor" href="#_2-编码器-解码器注意力"><span>2. 编码器-解码器注意力</span></a></h3><ul><li>Query来自前一层解码器的输出</li><li>Key和Value来自编码器的输出</li></ul><h2 id="transformer优势" tabindex="-1"><a class="header-anchor" href="#transformer优势"><span>Transformer优势</span></a></h2><h3 id="_1-并行化" tabindex="-1"><a class="header-anchor" href="#_1-并行化"><span>1. 并行化</span></a></h3><ul><li>相比RNN，可以并行处理序列中的所有位置</li><li>大幅提高训练效率</li></ul><h3 id="_2-长距离依赖" tabindex="-1"><a class="header-anchor" href="#_2-长距离依赖"><span>2. 长距离依赖</span></a></h3><ul><li>注意力机制可以直接连接任意两个位置</li><li>更好地处理长序列</li></ul><h3 id="_3-可解释性" tabindex="-1"><a class="header-anchor" href="#_3-可解释性"><span>3. 可解释性</span></a></h3><ul><li>注意力权重提供了模型决策的可视化</li><li>有助于理解模型行为</li></ul><h2 id="transformer变体" tabindex="-1"><a class="header-anchor" href="#transformer变体"><span>Transformer变体</span></a></h2><h3 id="_1-bert-bidirectional-encoder-representations-from-transformers" tabindex="-1"><a class="header-anchor" href="#_1-bert-bidirectional-encoder-representations-from-transformers"><span>1. BERT（Bidirectional Encoder Representations from Transformers）</span></a></h3><ul><li>只使用编码器部分</li><li>双向上下文理解</li><li>预训练+微调范式</li></ul><h3 id="_2-gpt-generative-pre-trained-transformer" tabindex="-1"><a class="header-anchor" href="#_2-gpt-generative-pre-trained-transformer"><span>2. GPT（Generative Pre-trained Transformer）</span></a></h3><ul><li>只使用解码器部分</li><li>单向语言模型</li><li>强大的文本生成能力</li></ul><h3 id="_3-t5-text-to-text-transfer-transformer" tabindex="-1"><a class="header-anchor" href="#_3-t5-text-to-text-transfer-transformer"><span>3. T5（Text-to-Text Transfer Transformer）</span></a></h3><ul><li>统一框架，所有NLP任务都视为文本到文本</li><li>编码器-解码器结构</li></ul><h2 id="transformer在计算机视觉中的应用" tabindex="-1"><a class="header-anchor" href="#transformer在计算机视觉中的应用"><span>Transformer在计算机视觉中的应用</span></a></h2><h3 id="_1-vision-transformer-vit" tabindex="-1"><a class="header-anchor" href="#_1-vision-transformer-vit"><span>1. Vision Transformer（ViT）</span></a></h3><ul><li>将图像划分为patches</li><li>将patches作为序列输入Transformer</li><li>在图像分类任务中取得优异性能</li></ul><h3 id="_2-swin-transformer" tabindex="-1"><a class="header-anchor" href="#_2-swin-transformer"><span>2. Swin Transformer</span></a></h3><ul><li>引入滑动窗口机制</li><li>适用于密集预测任务（如目标检测、语义分割）</li></ul><h2 id="优化与改进" tabindex="-1"><a class="header-anchor" href="#优化与改进"><span>优化与改进</span></a></h2><h3 id="_1-efficient-attention" tabindex="-1"><a class="header-anchor" href="#_1-efficient-attention"><span>1. Efficient Attention</span></a></h3><ul><li>Sparse Transformer：稀疏注意力</li><li>Linformer：线性复杂度注意力</li><li>Performer：基于核的快速注意力</li></ul><h3 id="_2-训练优化" tabindex="-1"><a class="header-anchor" href="#_2-训练优化"><span>2. 训练优化</span></a></h3><ul><li>学习率预热（Learning Rate Warmup）</li><li>标签平滑（Label Smoothing）</li><li>梯度裁剪（Gradient Clipping）</li></ul><h2 id="实现要点" tabindex="-1"><a class="header-anchor" href="#实现要点"><span>实现要点</span></a></h2><h3 id="_1-初始化" tabindex="-1"><a class="header-anchor" href="#_1-初始化"><span>1. 初始化</span></a></h3><ul><li>权重初始化对训练稳定性很重要</li><li>通常使用 Xavier 或 He 初始化</li></ul><h3 id="_2-正则化" tabindex="-1"><a class="header-anchor" href="#_2-正则化"><span>2. 正则化</span></a></h3><ul><li>Dropout防止过拟合</li><li>在注意力权重和残差连接后应用</li></ul><h3 id="_3-批处理" tabindex="-1"><a class="header-anchor" href="#_3-批处理"><span>3. 批处理</span></a></h3><ul><li>使用掩码处理不同长度的序列</li><li>提高计算效率</li></ul><h2 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景"><span>应用场景</span></a></h2><h3 id="_1-自然语言处理" tabindex="-1"><a class="header-anchor" href="#_1-自然语言处理"><span>1. 自然语言处理</span></a></h3><ul><li>机器翻译</li><li>文本摘要</li><li>问答系统</li><li>命名实体识别</li></ul><h3 id="_2-语音处理" tabindex="-1"><a class="header-anchor" href="#_2-语音处理"><span>2. 语音处理</span></a></h3><ul><li>语音识别</li><li>语音合成</li></ul><h3 id="_3-计算机视觉" tabindex="-1"><a class="header-anchor" href="#_3-计算机视觉"><span>3. 计算机视觉</span></a></h3><ul><li>图像分类</li><li>目标检测</li><li>图像生成</li></ul><p>Transformer架构的出现彻底改变了深度学习领域，特别是自然语言处理领域。它不仅解决了传统RNN的诸多问题，还为后续的大模型发展奠定了基础。</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated:: </span><time class="meta-item-info" datetime="2025-12-07T07:07:49.000Z" data-allow-mismatch>12/7/25, 7:07 AM</time></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: 1127699551@qq.com">mingwzh</span><!----><!--]--><!--]--></span></div></div></footer><nav class="vp-page-nav" aria-label="page navigation"><a class="route-link auto-link prev" href="/llm/llm-notes.html" aria-label="大模型(LLM)学习笔记"><!--[--><div class="hint"><span class="arrow left"></span> Prev</div><div class="link"><span class="external-link">大模型(LLM)学习笔记</span></div><!--]--></a><a class="route-link auto-link next" href="/llm/prompt-engineering.html" aria-label="提示工程(Prompt Engineering)学习笔记"><!--[--><div class="hint">Next <span class="arrow right"></span></div><div class="link"><span class="external-link">提示工程(Prompt Engineering)学习笔记</span></div><!--]--></a></nav><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-BvwJZ6kh.js" defer></script>
  </body>
</html>
